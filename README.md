# SentenceVAE:  
Input: Text document containing multiple sentences

Output: Reconstructed text generated sentence-by-sentence using SentenceVAE and OPT

# 1. Input Segmentation (Sentence Splitting)
function split_into_sentences(text):
    sentences = []  # Initialize an empty list to store sentences
    punctuation_marks = [".", "?", "!", ","]  # Define punctuation marks for splitting
    current_sentence = ""
    
    for char in text:
        current_sentence += char
        if char in punctuation_marks:
            sentences.append(current_sentence.strip())  # Add sentence to the list
            current_sentence = ""  # Reset for the next sentence
    if current_sentence:
        sentences.append(current_sentence.strip())  # Append any leftover sentence
    
    return sentences

# 2. Sentence Encoder with Learned Positional Encoding and Byte-Level BPE
function sentence_encoder(sentence):
    # Tokenize the sentence into word-level tokens using byte-level BPE
    tokens = byte_level_BPE(sentence)
    
    # Pass tokens through the OPT model's embedding layer
    embeddings = embedding_layer(tokens)
    
    # Add learned positional embeddings to word-level embeddings
    position_ids = create_position_ids(tokens)  # Generate position indices for each token
    positional_embeddings = positional_embedding_layer(position_ids)
    
    # Combine embeddings with positional embeddings
    embeddings_with_position = embeddings + positional_embeddings

    # Pass embeddings through self-attention-based encoder blocks of the OPT model
    hidden_features = encoder_blocks(embeddings_with_position)
    
    # Apply feature fusion to get the sentence-level token
    sentence_level_token = layer_norm(sum(hidden_features))  # Sum all hidden states and normalize
    
    return sentence_level_token

# 3. Sentence Decoder
function sentence_decoder(sentence_level_token):
    decoded_sentence = []
    token = "<bos>"  # Initialize with the beginning of sequence token
    
    while token != "<eos>":  # Loop until the end of sequence token is generated
        # Use sentence-level token and previous tokens to generate the next token
        token = decoder_blocks([decoded_sentence, token], sentence_level_token)
        decoded_sentence.append(token)
    
    return detokenize(decoded_sentence)  # Convert tokens back into a sentence

# 4. Training Process with Focal Loss, AdamW Optimizer, and Conditional Parallel Training
function train(sentence_encoder, sentence_decoder, sentences):
    optimizer = AdamW()  # Initialize AdamW optimizer
    device = "cuda" if torch.cuda.is_available() else "cpu"  # Check for GPU availability
    
    for sentence in sentences:
        # Move model and data to device (GPU if available, else CPU)
        sentence_level_token = sentence_encoder(sentence).to(device)
        
        # Parallel training with masked self-attention if GPU is available
        if device == "cuda":
            model = torch.nn.DataParallel(model)  # Wrap model for parallel GPU training
        
        # Decode the sentence and calculate the focal loss
        predicted_sentence = sentence_decoder(sentence_level_token).to(device)
        loss = focal_loss(predicted_sentence, sentence)

        # Backpropagation and parameter update
        loss.backward()
        optimizer.step()

# 5. Inference Process
function inference(text):
    # Split the input text into sentences
    sentences = split_into_sentences(text)
    
    reconstructed_text = ""
    
    for sentence in sentences:
        # Encode each sentence into a sentence-level token
        sentence_level_token = sentence_encoder(sentence)
        
        # Decode the sentence-level token back into the original sentence
        decoded_sentence = sentence_decoder(sentence_level_token)
        
        # Append the decoded sentence to the final output
        reconstructed_text += decoded_sentence + " "
    
    return reconstructed_text.strip()  # Return the reconstructed text

# 6. Integration into OPT (SLLM Model)
function integrate_sentenceVAE_with_OPT(text):
    # Discard the word-level embedding layers of the OPT model
    
    # Use sentence-level tokens generated by SentenceVAE as input to OPT decoder blocks
    sentences = split_into_sentences(text)
    
    for sentence in sentences:
        sentence_level_token = sentence_encoder(sentence)  # Get sentence-level token
        
        # Feed sentence-level token into OPT decoder blocks (no need for traditional embedding layer)
        output_token = OPT_decoder(sentence_level_token)
        
        if is_end_of_sentence(output_token):  # Use termination judgment layer
            break
    
    return output_token

# Main function to demonstrate end-to-end process
function main(text):
    # Inference: Process text sentence-by-sentence using SentenceVAE and OPT
    result = inference(text)
    return result

# Example usage
text = "This is a test. How does it work?"
output = main(text)
print(output)


# 7. Citation

[link](https://arxiv.org/abs/2408.00655)

An, Hongjun, et al. "SentenceVAE: Faster, Longer and More Accurate Inference with Next-sentence Prediction for Large Language Models." arXiv preprint arXiv:2408.00655 (2024).
